{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\nimport datetime\nimport random\nimport gc\nimport warnings\nfrom joblib import dump, load\nimport uuid\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearnex import patch_sklearn\npatch_sklearn()\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, jaccard_score, recall_score, precision_score, accuracy_score, make_scorer, roc_auc_score, f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-08-23T00:29:49.009407Z","iopub.execute_input":"2022-08-23T00:29:49.009928Z","iopub.status.idle":"2022-08-23T00:29:51.785417Z","shell.execute_reply.started":"2022-08-23T00:29:49.009823Z","shell.execute_reply":"2022-08-23T00:29:51.784137Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stdout","text":"CPU times: user 1.49 s, sys: 456 ms, total: 1.95 s\nWall time: 2.74 s\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_date_sort(frame : pd.DataFrame, primary_key : str, date_types : list = None):\n        df = frame.copy()\n        if date_types is not None :\n            for date_col in date_types :\n                df[date_col] = pd.to_datetime(df[date_col])\n            df = df.sort_values([primary_key]+date_types)\n        else :\n            df = df.sort_values([primary_key])\n        return df\n\ndef feature_engineer_test(frame :pd.DataFrame, cat_features : list, primary_key : str):\n        num_features = [x for x in list(frame.select_dtypes('number')) and x not in cat_features]\n        test_num_agg = frame.groupby(primary_key)[num_features].agg(['first','mean', 'std', 'min', 'max', 'last'])\n        test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n\n        # Diff/Div columns\n        for col in test_num_agg.columns:  \n            # Last/First\n            if 'last' in col and col.replace('last', 'first') in test_num_agg.columns:\n                test_num_agg[col + '_life_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', 'first')]\n            if 'last' in col and col.replace('last', 'mean') in test_num_agg.columns:\n                test_num_agg[col + '_lmean_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', 'mean')]\n\n        test_cat_agg = frame.groupby(primary_key)[cat_features].agg(['first', 'last', 'nunique'])\n        test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n\n        temp1 = frame.groupby([primary_key])['P_2'].count()\n        temp1 = temp1.reset_index()\n        temp1.columns = [primary_key,'num_statements']\n        temp1 = temp1.set_index(primary_key)\n\n        df = pd.concat([test_num_agg, test_cat_agg,temp1], axis=1) #test_bal_agg  \n        df = df.reset_index()\n\n        del test_num_agg, test_cat_agg, temp1\n        gc.collect()\n\n        return df\n    \ndef predictor(model, frame : pd.DataFrame) :\n    #X = frame[frame.columns.tolist()[1:]].values\n    X = frame.values\n    pred_proba = model.predict_proba(X)[ : , 1]\n    pred_class = model.predict(X)\n    submission1  = pd.DataFrame({'customer_ID' : df[df.columns.tolist()[0]].values, 'prediction' : pred_proba})\n    submission2  = pd.DataFrame({'customer_ID' : df[df.columns.tolist()[0]].values, 'prediction' : pred_class})\n    del pred_proba, pred_class\n    gc.collect()\n    print('Predictions completed successfully ðŸ‘Œ')\n    return [submission1, submission2]\n\n\ndef compute_scores(model , X_val) :\n    pred = model.predict(X_val)\n    pred_proba = model.predict_proba(X_val)[:,1]\n    confusion = confusion_matrix(Y_val,pred.copy(), labels = model.classes_)\n    \n    scores = dict(acc = accuracy_score(Y_val,pred.copy()),\n                      jaccard = jaccard_score(Y_val, pred.copy()), \n                      recall = recall_score(Y_val, pred.copy()), \n                      prec = precision_score(Y_val, pred.copy()), \n                      auc = roc_auc_score(Y_val, pred_proba.copy()), \n                      f1 = f1_score(Y_val, pred.copy()), \n                      comp = amex_sk(Y_val,pred_proba), \n                      tn = confusion[0,0]/sum(confusion[0]), \n                      tp = confusion[1,1]/sum(confusion[1]))\n    return scores\n\n\ndef amex_sk(target: np.ndarray, preds: np.ndarray) -> float:\n    n_pos = np.sum(target)\n    n_neg = target.shape[0] - n_pos\n\n    indices = np.argsort(preds)[::-1]\n    preds, target = preds[indices], target[indices]\n\n    weight = 20.0 - target * 19.0\n    cum_norm_weight = (weight * (1 / weight.sum())).cumsum()\n    four_pct_mask = cum_norm_weight <= 0.04\n    d = np.sum(target[four_pct_mask]) / n_pos\n\n    lorentz = (target * (1 / n_pos)).cumsum()\n    gini = ((lorentz - cum_norm_weight) * weight).sum()\n\n    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n\n    g = gini / gini_max\n    return 0.5 * (g + d)\n\n\n\ndef export_results(model, sub : list((pd.DataFrame,pd.DataFrame)) = None) :\n    uid = str(uuid.uuid4())\n    dump(model, '/kaggle/input/model_amex_'+uid+'joblib')\n    if sub is not None :\n        sub[0].to_csv('/kaggle/input/subimission_proba_amex_'+str(uid)+'.csv', index =False)\n        sub[1].to_csv('/kaggle/input/subimission_class_amex_'+str(uid)+'.csv', index =False)\n        print('Subimission completed successfully ðŸ‘Œ')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/home/drxc/.kaggle/Kaggle_competitions/Datasets/Amex-Default-Prediction') # set the wd to /home/drxc/.kaggle/Kaggle_competitions/Datasets/Amex-Default-Prediction\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_data = pd.read_parquet('train_data.parquet')\nprint(train_data.shape)\nfeatures = load('features.csv')\ntest_data = pd.read_parquet('test_data.parquet')\nlabels = pd.read_csv('train_labels.csv')\ncat = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata = pd.concat([train_data, test_data], ignore_index=True)\ndel train_data, test_data\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnacount = data.isna().any().count()/100\nnullcols = nacount[nacount>= 30]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata = data.drop(columns = nullcols)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = convert_date_sort(frame = data, primary_key = 'customer_ID', date_types = ['S_2'])\ndata = feature_engineer_test(frame = data, cat_features = cat, primary_key = 'customer_ID')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test_data = convert_date_sort(frame = test_data, primary_key = 'customer_ID', date_types = ['S_2'])\ntest_data = feature_engineer_test(frame = test_data, cat_features = cat, primary_key = 'customer_ID')\nprint(test_data.shape)\ndata = pd.concat([train_data, test_data], ignore_index=True)\ndel train_data, test_data","metadata":{}},{"cell_type":"code","source":"%%time\nencoder = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = np.nan)\nscaler = MinMaxScaler()\n\ndata[data.columns[1:]] = encoder.fit_transform(data[data.columns[1:]])\ndata[data.columns[1:]] = data[data.columns[1:]].fillna(data[data.columns[1:]].median()) # faire aussi aprÃ¨s sÃ©paration train/test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#data[data.columns[1:]] = scaler.fit_transform(data[data.columns[1:]]) # faire aussi aprÃ¨s sÃ©paration train/test/val\ntrain_data = pd.merge(data, labels, on = 'customer_ID', how = 'inner')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest_data = data.set_index('customer_ID').drop(index=labels.custom_ID.unique().tolist()).reset_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_train, X_val, y_train, y_val = train_test_split(X = train_data[data.columns[1:-1]], y=train_data[data.columns[-1]] , test_size=0.1, random_state=43)\nX_test = test_data[data.columns[1:]]\ndel train_data, data, test_data\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb = XGBClassifier(base_score = 0.7, booster = 'gbtree', gamma = 10, \n                    importance_type = 'weight', learning_rate = 0.1, max_leaves = 60, \n                    n_jobs = -1, random_state = 43, reg_alpha = 0.3, \n                    reg_lambda = 0.7,  subsample = 0.5, sampling_method= 'uniform', \n                    verbosity = 3, tree_method = 'hist', objective  = 'reg:logistic', seed = 43)\n\nxgb = xgb.fit(X_train, y_train)\n\nscores = compute_scores(xgb , X_val)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsub = predictor(xgb, X_test)\nexport_results(xgb, sub = sub)","metadata":{},"execution_count":null,"outputs":[]}]}